# test_timing_detail.py
"""ê° ì²˜ë¦¬ ë‹¨ê³„ë³„ ìƒì„¸ ì‹œê°„ ì¸¡ì •"""

import asyncio
import logging
import sys
import os
import time
from datetime import datetime
from pathlib import Path
import httpx
from typing import Dict, Any, List
import re

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from infra.core.config import settings
from infra.databases.mongo_db import MongoDB

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DetailedTimingTester:
    """ìƒì„¸ íƒ€ì´ë° í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.api_url = f"http://localhost:{settings.API_PORT}"
        self.mongo = MongoDB()
        
    def create_test_pdf(self, name: str, chunks: int) -> Path:
        """íŠ¹ì • ì²­í¬ ìˆ˜ë¥¼ ìƒì„±í•˜ëŠ” PDF"""
        try:
            from reportlab.pdfgen import canvas
            from reportlab.lib.pagesizes import letter
            from reportlab.lib.units import inch
        except ImportError:
            logger.error("reportlab not installed")
            return None
        
        test_dir = Path("test_timing")
        test_dir.mkdir(exist_ok=True)
        pdf_path = test_dir / f"{name}.pdf"
        
        c = canvas.Canvas(str(pdf_path), pagesize=letter)
        width, height = letter
        
        # ì œëª©
        c.setFont("Helvetica-Bold", 20)
        c.drawString(inch, height - inch, f"Timing Test - {chunks} chunks expected")
        
        # ì²­í¬ í¬ê¸°ë¥¼ ê³ ë ¤í•œ ë‚´ìš© ìƒì„± (chunk_size=1000, overlap=200)
        c.setFont("Helvetica", 11)
        y = height - 2*inch
        
        # ê° ì²­í¬ëŠ” ì•½ 800ì ì •ë„ì˜ ìœ íš¨ í…ìŠ¤íŠ¸ í•„ìš” (ì˜¤ë²„ë© ê³ ë ¤)
        text_per_chunk = 800
        total_text_needed = text_per_chunk * chunks
        
        # í…ìŠ¤íŠ¸ ìƒì„±
        text_lines = []
        for i in range(chunks):
            text_lines.append(f"## Section {i+1}")
            text_lines.append("")
            # ê° ì„¹ì…˜ì— ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ ì¶”ê°€
            paragraph = f"This is section {i+1} of the timing test document. " * 20
            text_lines.append(paragraph)
            text_lines.append("")
        
        # PDFì— ì“°ê¸°
        for line in text_lines:
            if y < inch:
                c.showPage()
                c.setFont("Helvetica", 11)
                y = height - inch
            
            if line.startswith("##"):
                c.setFont("Helvetica-Bold", 14)
                c.drawString(inch, y, line)
                c.setFont("Helvetica", 11)
                y -= 20
            else:
                # ê¸´ í…ìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ
                words = line.split()
                current_line = ""
                for word in words:
                    if len(current_line + word) > 70:
                        c.drawString(inch, y, current_line)
                        y -= 15
                        current_line = word + " "
                    else:
                        current_line += word + " "
                if current_line:
                    c.drawString(inch, y, current_line)
                    y -= 15
        
        c.save()
        return pdf_path
    
    async def analyze_processing_logs(self, document_id: str) -> Dict[str, Any]:
        """ì²˜ë¦¬ ë¡œê·¸ ë¶„ì„"""
        # Processing ë¡œê·¸ íŒŒì¼ ì½ê¸° (ì„¤ì •ì— ë”°ë¼)
        if settings.PROCESSING_LOG_MODE in ["file", "both"]:
            log_file = Path(settings.PROCESSING_LOG_FILE)
            if log_file.exists():
                with open(log_file, 'r') as f:
                    logs = f.readlines()
                
                # í•´ë‹¹ document_idì— ëŒ€í•œ ë¡œê·¸ í•„í„°ë§
                doc_logs = [l for l in logs if document_id in l]
                
                # íƒ€ì´ë° ì •ë³´ ì¶”ì¶œ
                timings = {}
                for log in doc_logs:
                    if "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ" in log:
                        match = re.search(r'duration=(\d+\.\d+)s', log)
                        if match:
                            timings['text_extraction'] = float(match.group(1))
                    
                    elif "ì²­í‚¹ ì™„ë£Œ" in log:
                        match = re.search(r'duration=(\d+\.\d+)s', log)
                        if match:
                            timings['chunking'] = float(match.group(1))
                    
                    elif "ì„ë² ë”© ìƒì„± ì™„ë£Œ" in log:
                        match = re.search(r'duration=(\d+\.\d+)s', log)
                        if match:
                            timings['embedding'] = float(match.group(1))
                
                return timings
        
        return {}
    
    async def test_single_pdf(self, chunks: int) -> Dict[str, Any]:
        """ë‹¨ì¼ PDF í…ŒìŠ¤íŠ¸"""
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“‹ Testing PDF with ~{chunks} chunks")
        logger.info(f"{'='*60}")
        
        # 1. PDF ìƒì„±
        pdf_path = self.create_test_pdf(f"test_{chunks}_chunks", chunks)
        if not pdf_path:
            return None
        
        file_size = pdf_path.stat().st_size
        logger.info(f"Created PDF: {pdf_path.name} ({file_size:,} bytes)")
        
        # 2. ì—…ë¡œë“œ
        upload_start = time.time()
        
        with open(pdf_path, 'rb') as f:
            pdf_content = f.read()
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            files = {'file': (pdf_path.name, pdf_content, 'application/pdf')}
            
            response = await client.post(
                f"{self.api_url}/api/v1/upload/pdf",
                files=files
            )
            
            upload_time = time.time() - upload_start
            
            if response.status_code != 200:
                logger.error(f"Upload failed: {response.status_code}")
                return None
            
            result = response.json()
            document_id = result['document_id']
            
        logger.info(f"âœ… Upload complete in {upload_time:.2f}s")
        
        # 3. ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§
        process_start = time.time()
        last_status = None
        
        while True:
            doc = await self.mongo.find_one("uploads", {"document_id": document_id})
            
            if doc:
                status = doc.get('status', 'unknown')
                
                if status != last_status:
                    elapsed = time.time() - process_start
                    logger.info(f"   Status: {last_status} â†’ {status} (at {elapsed:.1f}s)")
                    last_status = status
                
                if status == 'completed':
                    process_time = time.time() - process_start
                    
                    # ì²˜ë¦¬ ì •ë³´
                    info = doc.get('processing_info', {})
                    actual_chunks = info.get('total_chunks', 0)
                    embeddings = info.get('embeddings_created', 0)
                    
                    logger.info(f"\nâœ… Processing completed:")
                    logger.info(f"   - Processing time: {process_time:.2f}s")
                    logger.info(f"   - Actual chunks: {actual_chunks}")
                    logger.info(f"   - Embeddings created: {embeddings}")
                    
                    # ì²« ë²ˆì§¸ ì²­í¬ì—ì„œ ìƒì„¸ ì •ë³´ í™•ì¸
                    chunk = await self.mongo.db.pdf_chunks.find_one(
                        {"document_id": document_id}
                    )
                    
                    if chunk and 'chunk_metadata' in chunk:
                        meta = chunk['chunk_metadata']
                        logger.info(f"\nğŸ“Š Processing details:")
                        logger.info(f"   - Chunking strategy: {meta.get('chunking_strategy')}")
                        logger.info(f"   - Embedding model: {meta.get('model', 'N/A')}")
                    
                    # ë¡œê·¸ ë¶„ì„
                    log_timings = await self.analyze_processing_logs(document_id)
                    if log_timings:
                        logger.info(f"\nâ±ï¸  Detailed timings from logs:")
                        for step, duration in log_timings.items():
                            logger.info(f"   - {step}: {duration:.2f}s")
                    
                    # ê²°ê³¼ ë°˜í™˜
                    return {
                        'expected_chunks': chunks,
                        'actual_chunks': actual_chunks,
                        'embeddings': embeddings,
                        'upload_time': upload_time,
                        'process_time': process_time,
                        'total_time': upload_time + process_time,
                        'file_size': file_size,
                        'log_timings': log_timings
                    }
                
                elif status == 'failed':
                    logger.error(f"Processing failed: {doc.get('error_message')}")
                    break
            
            if time.time() - process_start > 60:
                logger.warning("Timeout after 60s")
                break
            
            await asyncio.sleep(1)
        
        return None
    
    async def run_timing_test(self):
        """íƒ€ì´ë° í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        # ë‹¤ì–‘í•œ í¬ê¸° í…ŒìŠ¤íŠ¸
        test_sizes = [5, 10, 20, 50]
        results = []
        
        for size in test_sizes:
            result = await self.test_single_pdf(size)
            if result:
                results.append(result)
            await asyncio.sleep(2)  # í…ŒìŠ¤íŠ¸ ê°„ ê°„ê²©
        
        # ê²°ê³¼ ë¶„ì„
        if results:
            logger.info(f"\n{'='*60}")
            logger.info("ğŸ“Š Timing Test Summary")
            logger.info(f"{'='*60}")
            
            logger.info("\n| Chunks | Upload | Process | Total | Embeddings | File Size |")
            logger.info("|--------|--------|---------|-------|------------|-----------|")
            
            for r in results:
                logger.info(
                    f"| {r['actual_chunks']:6} | {r['upload_time']:6.2f}s | {r['process_time']:7.2f}s | "
                    f"{r['total_time']:5.2f}s | {r['embeddings']:10} | {r['file_size']:9,} |"
                )
            
            # ì„ë² ë”© ì‹œê°„ ë¶„ì„
            logger.info("\nğŸ“ˆ Processing Time Breakdown:")
            for i, r in enumerate(results):
                logger.info(f"\n{r['actual_chunks']} chunks:")
                logger.info(f"  - Total processing: {r['process_time']:.2f}s")
                
                if r.get('log_timings'):
                    timings = r['log_timings']
                    if 'embedding' in timings:
                        embed_pct = (timings['embedding'] / r['process_time']) * 100
                        logger.info(f"  - Embedding time: {timings['embedding']:.2f}s ({embed_pct:.1f}%)")
                    if 'text_extraction' in timings:
                        logger.info(f"  - Text extraction: {timings['text_extraction']:.2f}s")
                    if 'chunking' in timings:
                        logger.info(f"  - Chunking: {timings['chunking']:.2f}s")
                
                # ì²­í¬ë‹¹ í‰ê·  ì‹œê°„
                if r['actual_chunks'] > 0:
                    avg_per_chunk = r['process_time'] / r['actual_chunks']
                    logger.info(f"  - Average per chunk: {avg_per_chunk:.3f}s")
        
        # ì •ë¦¬
        test_dir = Path("test_timing")
        if test_dir.exists():
            import shutil
            shutil.rmtree(test_dir)
            logger.info("\nğŸ§¹ Cleaned up test files")
    
    async def check_embedding_config(self):
        """ì„ë² ë”© ì„¤ì • í™•ì¸"""
        logger.info("\nğŸ”§ Embedding Configuration:")
        logger.info(f"  - Model: {settings.OPENAI_EMBEDDING_MODEL}")
        logger.info(f"  - Batch size: {settings.PDF_BATCH_SIZE}")
        logger.info(f"  - Sub-batch size: {settings.PDF_SUB_BATCH_SIZE}")
        logger.info(f"  - Max concurrent API calls: {settings.PDF_MAX_CONCURRENT_API_CALLS}")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # API ì„œë²„ í™•ì¸
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"http://localhost:{settings.API_PORT}/health")
            if response.status_code != 200:
                logger.error("âŒ API server not running")
                return
            logger.info("âœ… API server is running")
    except:
        logger.error("âŒ Cannot connect to API server")
        return
    
    # ë¡œê¹… ëª¨ë“œ í™•ì¸
    logger.info(f"\nğŸ“ Processing log mode: {settings.PROCESSING_LOG_MODE}")
    if settings.PROCESSING_LOG_MODE not in ["file", "both"]:
        logger.warning("âš ï¸  Detailed timing logs only available with file logging enabled")
        logger.info("   Set PROCESSING_LOG_MODE=both in .env.development for detailed timings")
    
    tester = DetailedTimingTester()
    
    # ì„ë² ë”© ì„¤ì • í™•ì¸
    await tester.check_embedding_config()
    
    # íƒ€ì´ë° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    await tester.run_timing_test()


if __name__ == "__main__":
    asyncio.run(main())