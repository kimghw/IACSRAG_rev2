# test_batch_upload.py
import asyncio
import logging
import sys
import os
import httpx
import json
from pathlib import Path
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from infra.core.config import settings

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BatchUploadTester:
    """ë°°ì¹˜ ì—…ë¡œë“œ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.api_url = f"http://localhost:{settings.API_PORT}"
        self.test_files = []
    
    def create_test_files(self):
        """ë‹¤ì–‘í•œ íƒ€ì…ì˜ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"""
        test_dir = Path("test_files")
        test_dir.mkdir(exist_ok=True)
        
        # 1. PDF íŒŒì¼ ìƒì„± (í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ëŒ€ì²´)
        pdf_content = """Test PDF Document
This is a test PDF document for batch upload testing.
It contains multiple lines of text.
The system should process this as a PDF file.
Created at: {}""".format(datetime.now())
        
        pdf_path = test_dir / "test_document.pdf"
        pdf_path.write_text(pdf_content)
        self.test_files.append(("test_document.pdf", pdf_content))
        
        # 2. Markdown íŒŒì¼ ìƒì„±
        md_content = """# Test Markdown Document

## Introduction
This is a test markdown document for batch upload testing.

### Features
- Bullet point 1
- Bullet point 2
- Bullet point 3

### Code Example
```python
def hello_world():
    print("Hello, World!")
```

Created at: {}""".format(datetime.now())
        
        md_path = test_dir / "test_document.md"
        md_path.write_text(md_content)
        self.test_files.append(("test_document.md", md_content))
        
        # 3. JSON íŒŒì¼ ìƒì„±
        json_data = {
            "title": "Test JSON Document",
            "description": "This is a test JSON document for batch upload testing",
            "data": {
                "items": ["item1", "item2", "item3"],
                "count": 3,
                "active": True
            },
            "created_at": datetime.now().isoformat()
        }
        json_content = json.dumps(json_data, indent=2)
        
        json_path = test_dir / "test_document.json"
        json_path.write_text(json_content)
        self.test_files.append(("test_document.json", json_content))
        
        # 4. ì¶”ê°€ íŒŒì¼ë“¤ (ë°°ì¹˜ í…ŒìŠ¤íŠ¸ìš©)
        for i in range(2):
            # ì¶”ê°€ PDF
            pdf_content_extra = f"Extra PDF Document {i+1}\nContent for testing batch upload\nFile number: {i+1}"
            pdf_path_extra = test_dir / f"extra_document_{i+1}.pdf"
            pdf_path_extra.write_text(pdf_content_extra)
            self.test_files.append((f"extra_document_{i+1}.pdf", pdf_content_extra))
            
            # ì¶”ê°€ Markdown
            md_content_extra = f"# Extra Markdown {i+1}\n\nContent for testing batch upload\n\nFile number: {i+1}"
            md_path_extra = test_dir / f"extra_document_{i+1}.md"
            md_path_extra.write_text(md_content_extra)
            self.test_files.append((f"extra_document_{i+1}.md", md_content_extra))
        
        logger.info(f"Created {len(self.test_files)} test files")
    
    async def test_batch_upload(self):
        """ë°°ì¹˜ ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸"""
        logger.info("\n" + "="*60)
        logger.info("ğŸš€ Starting Batch Upload Test")
        logger.info("="*60)
        
        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        metadata = {
            "test_run": True,
            "test_timestamp": datetime.now().isoformat(),
            "source": "batch_upload_test"
        }
        
        # íŒŒì¼ ì¤€ë¹„
        files = []
        for filename, content in self.test_files:
            files.append(('files', (filename, content.encode('utf-8'))))
        
        # ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤í–‰
        async with httpx.AsyncClient(timeout=30.0) as client:
            logger.info(f"\nğŸ“¤ Uploading {len(self.test_files)} files in batch...")
            
            response = await client.post(
                f"{self.api_url}/api/v1/upload/batch",
                files=files,
                data={'metadata': json.dumps(metadata)}
            )
            
            if response.status_code == 200:
                result = response.json()
                
                logger.info("\nâœ… Batch Upload Successful!")
                logger.info(f"   Total Files: {result['total_files']}")
                logger.info(f"   Successful: {result['successful_count']}")
                logger.info(f"   Failed: {result['failed_count']}")
                
                # ì„±ê³µí•œ ì—…ë¡œë“œ ì •ë³´
                if result['successful_uploads']:
                    logger.info("\nğŸ“‹ Successful Uploads:")
                    for upload in result['successful_uploads']:
                        logger.info(f"   - {upload['filename']} (ID: {upload['document_id'][:8]}...)")
                
                # ì‹¤íŒ¨í•œ ì—…ë¡œë“œ ì •ë³´
                if result['failed_uploads']:
                    logger.info("\nâŒ Failed Uploads:")
                    for upload in result['failed_uploads']:
                        logger.info(f"   - {upload['filename']}: {upload['error']}")
                
                return result['successful_uploads']
            else:
                logger.error(f"âŒ Batch upload failed: {response.status_code}")
                logger.error(f"   Response: {response.text}")
                return None
    
    async def monitor_kafka_events(self, document_ids: list, duration: int = 10):
        """Kafka ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§"""
        logger.info(f"\nğŸ” Monitoring Kafka events for {len(document_ids)} documents")
        
        from aiokafka import AIOKafkaConsumer
        
        consumer = AIOKafkaConsumer(
            settings.KAFKA_TOPIC_DOCUMENT_UPLOADED,
            bootstrap_servers=settings.KAFKA_BOOTSTRAP_SERVERS,
            group_id=f"test-batch-monitor-{datetime.now().timestamp()}",
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            auto_offset_reset='latest'
        )
        
        await consumer.start()
        
        try:
            end_time = asyncio.get_event_loop().time() + duration
            event_count = 0
            event_types = {}
            
            while asyncio.get_event_loop().time() < end_time:
                try:
                    records = await asyncio.wait_for(
                        consumer.getmany(timeout_ms=1000),
                        timeout=1.5
                    )
                    
                    for topic_partition, messages in records.items():
                        for msg in messages:
                            doc_id = msg.value.get('document_id')
                            if doc_id in [doc['document_id'] for doc in document_ids]:
                                event_count += 1
                                event_type = msg.value.get('event_type', 'unknown')
                                event_types[event_type] = event_types.get(event_type, 0) + 1
                                
                                logger.info(f"\nğŸ“¨ Event #{event_count} received:")
                                logger.info(f"   Type: {event_type}")
                                logger.info(f"   Document: {doc_id[:8]}...")
                                logger.info(f"   Filename: {msg.value.get('filename')}")
                                
                except asyncio.TimeoutError:
                    continue
            
            logger.info(f"\nğŸ“Š Event Summary:")
            logger.info(f"   Total Events: {event_count}")
            for event_type, count in event_types.items():
                logger.info(f"   - {event_type}: {count}")
                
        finally:
            await consumer.stop()
    
    async def run_full_test(self):
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            # 1. í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
            self.create_test_files()
            
            # 2. ë°°ì¹˜ ì—…ë¡œë“œ ì‹¤í–‰
            successful_uploads = await self.test_batch_upload()
            
            if successful_uploads:
                # 3. Kafka ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§
                await self.monitor_kafka_events(successful_uploads, duration=10)
            
            logger.info("\n" + "="*60)
            logger.info("âœ… Batch Upload Test Completed!")
            logger.info("="*60)
            
        finally:
            # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
            self.cleanup_test_files()
    
    def cleanup_test_files(self):
        """í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬"""
        test_dir = Path("test_files")
        if test_dir.exists():
            import shutil
            shutil.rmtree(test_dir)
            logger.info("ğŸ§¹ Cleaned up test files")

async def test_simple_batch():
    """ê°„ë‹¨í•œ ë°°ì¹˜ ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    logger.info("\nğŸš€ Simple Batch Upload Test")
    
    # ê°„ë‹¨í•œ íŒŒì¼ 3ê°œ ì¤€ë¹„
    files = [
        ('files', ('simple1.pdf', b'Simple PDF content 1')),
        ('files', ('simple2.md', b'# Simple Markdown\n\nContent 2')),
        ('files', ('simple3.json', b'{"test": true, "value": 3}'))
    ]
    
    metadata = {
        "test": "simple_batch",
        "timestamp": datetime.now().isoformat()
    }
    
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"http://localhost:{settings.API_PORT}/api/v1/upload/batch",
            files=files,
            data={'metadata': json.dumps(metadata)}
        )
        
        if response.status_code == 200:
            result = response.json()
            logger.info(f"âœ… Success! Total files: {result['total_files']}")
            logger.info(f"   Uploaded: {result['successful_count']}")
            logger.info(f"   Failed: {result['failed_count']}")
            
            # ê°œë³„ ê²°ê³¼ ì¶œë ¥
            for upload in result['successful_uploads']:
                logger.info(f"   âœ“ {upload['filename']} â†’ {upload['document_id'][:8]}...")
        else:
            logger.error(f"âŒ Failed: {response.status_code}")
            logger.error(f"   {response.text}")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # API ì„œë²„ í™•ì¸
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"http://localhost:{settings.API_PORT}/health")
            if response.status_code != 200:
                logger.error("âš ï¸  API server is not running. Please start it first.")
                return
    except:
        logger.error("âŒ API server is not running. Please start it first:")
        logger.error("   python main.py")
        return
    
    # í…ŒìŠ¤íŠ¸ ì„ íƒ
    if len(sys.argv) > 1 and sys.argv[1] == "simple":
        await test_simple_batch()
    else:
        # ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        tester = BatchUploadTester()
        await tester.run_full_test()

if __name__ == "__main__":
    asyncio.run(main())