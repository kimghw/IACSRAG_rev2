#!/usr/bin/env python3
"""
SpaCy ì²­í‚¹ ê²°ê³¼ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
MongoDBì—ì„œ ìµœê·¼ ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ì²­í‚¹ ê²°ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
"""
import asyncio
from motor.motor_asyncio import AsyncIOMotorClient
from datetime import datetime
import statistics

# MongoDB ì—°ê²° ì„¤ì •
MONGODB_URL = 'mongodb://admin:password@localhost:27017'
MONGODB_DATABASE = 'iacsrag_dev'

async def analyze_spacy_chunks():
    """SpaCyë¡œ ì²˜ë¦¬ëœ ì²­í¬ë“¤ì„ ë¶„ì„"""
    client = AsyncIOMotorClient(MONGODB_URL)
    db = client[MONGODB_DATABASE]
    
    print("ğŸ” MongoDB ì—°ê²° ì„±ê³µ")
    print("="*80)
    
    # ê°€ì¥ ìµœê·¼ ì—…ë¡œë“œëœ ë¬¸ì„œ ì°¾ê¸°
    latest_doc = await db.uploads.find_one(
        {},
        sort=[("uploaded_at", -1)]
    )
    
    if not latest_doc:
        print("âŒ ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    document_id = latest_doc['document_id']
    print(f"ğŸ“„ ë¶„ì„í•  ë¬¸ì„œ ì •ë³´:")
    print(f"   - Document ID: {document_id}")
    print(f"   - íŒŒì¼ëª…: {latest_doc['filename']}")
    print(f"   - ì—…ë¡œë“œ ì‹œê°„: {latest_doc['uploaded_at']}")
    print(f"   - ìƒíƒœ: {latest_doc.get('status', 'N/A')}")
    print("\n" + "="*80 + "\n")
    
    # í•´ë‹¹ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸°
    total_chunks = await db.pdf_chunks.count_documents({'document_id': document_id})
    print(f"ğŸ“Š ì´ ì²­í¬ ìˆ˜: {total_chunks}")
    
    if total_chunks == 0:
        print("âŒ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì²˜ìŒ 5ê°œ ì²­í¬ ìƒì„¸ ì •ë³´
    print(f"\nğŸ“ ì²˜ìŒ 5ê°œ ì²­í¬ ìƒì„¸ ì •ë³´:")
    print("-"*80)
    
    chunks_sample = await db.pdf_chunks.find(
        {"document_id": document_id},
        sort=[("chunk_index", 1)]
    ).to_list(length=5)
    
    for chunk in chunks_sample:
        print(f"\nğŸ”¸ ì²­í¬ #{chunk['chunk_index']}")
        print(f"   ID: {chunk['_id']}")
        print(f"   í¬ê¸°: {chunk['char_end'] - chunk['char_start']} ë¬¸ì")
        
        # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
        text_preview = chunk['text_content'][:150].replace('\n', ' ')
        print(f"   í…ìŠ¤íŠ¸: {text_preview}...")
        
        # ë©”íƒ€ë°ì´í„° í™•ì¸
        metadata = chunk.get('chunk_metadata', {})
        print(f"\n   ğŸ“‹ ë©”íƒ€ë°ì´í„°:")
        print(f"      - ì²­í‚¹ ì „ëµ: {metadata.get('chunking_strategy', 'N/A')}")
        print(f"      - ì²­í¬ í¬ê¸°: {metadata.get('chunk_size', 'N/A')}")
        
        # SpaCy íŠ¹ìœ ì˜ ì •ë³´ (entities, topics)
        entities = metadata.get('entities', [])
        topics = metadata.get('topics', [])
        
        print(f"      - ì—”í‹°í‹°: {len(entities)}ê°œ")
        if entities:
            print(f"        â†’ {', '.join(entities[:5])}{'...' if len(entities) > 5 else ''}")
        
        print(f"      - í† í”½: {len(topics)}ê°œ")
        if topics:
            print(f"        â†’ {', '.join(topics[:10])}{'...' if len(topics) > 10 else ''}")
        
        # ì„ë² ë”© ìƒíƒœ
        embedding_status = chunk.get('embedding_status', 'N/A')
        embedding_id = chunk.get('embedding_id', None)
        print(f"      - ì„ë² ë”© ìƒíƒœ: {embedding_status}")
        if embedding_id:
            print(f"      - ì„ë² ë”© ID: {embedding_id[:16]}...")
    
    print("\n" + "="*80 + "\n")
    
    # ì „ì²´ í†µê³„ ë¶„ì„
    print("ğŸ“Š ì „ì²´ ì²­í‚¹ í†µê³„:")
    print("-"*80)
    
    # ëª¨ë“  ì²­í¬ ê°€ì ¸ì™€ì„œ ë¶„ì„
    all_chunks = await db.pdf_chunks.find(
        {"document_id": document_id}
    ).to_list(length=None)
    
    # ì²­í¬ í¬ê¸° ë¶„ì„
    chunk_sizes = [chunk['char_end'] - chunk['char_start'] for chunk in all_chunks]
    avg_size = sum(chunk_sizes) / len(chunk_sizes)
    median_size = statistics.median(chunk_sizes)
    
    print(f"   ğŸ“ í¬ê¸° ë¶„ì„:")
    print(f"      - í‰ê·  í¬ê¸°: {avg_size:.0f} ë¬¸ì")
    print(f"      - ì¤‘ê°„ê°’: {median_size:.0f} ë¬¸ì")
    print(f"      - ìµœì†Œ í¬ê¸°: {min(chunk_sizes)} ë¬¸ì")
    print(f"      - ìµœëŒ€ í¬ê¸°: {max(chunk_sizes)} ë¬¸ì")
    print(f"      - í‘œì¤€í¸ì°¨: {statistics.stdev(chunk_sizes):.0f} ë¬¸ì")
    
    # ì—”í‹°í‹°/í† í”½ ë¶„ì„
    total_entities = 0
    total_topics = 0
    chunks_with_entities = 0
    chunks_with_topics = 0
    all_entities = set()
    all_topics = set()
    
    for chunk in all_chunks:
        metadata = chunk.get('chunk_metadata', {})
        entities = metadata.get('entities', [])
        topics = metadata.get('topics', [])
        
        if entities:
            chunks_with_entities += 1
            total_entities += len(entities)
            all_entities.update(entities)
        
        if topics:
            chunks_with_topics += 1
            total_topics += len(topics)
            all_topics.update(topics)
    
    print(f"\n   ğŸ·ï¸ ì—”í‹°í‹° ë¶„ì„:")
    print(f"      - ì—”í‹°í‹°ê°€ ìˆëŠ” ì²­í¬: {chunks_with_entities}/{len(all_chunks)} ({chunks_with_entities/len(all_chunks)*100:.1f}%)")
    print(f"      - ì´ ì—”í‹°í‹° ìˆ˜: {total_entities}")
    print(f"      - ê³ ìœ  ì—”í‹°í‹° ìˆ˜: {len(all_entities)}")
    if chunks_with_entities > 0:
        print(f"      - ì²­í¬ë‹¹ í‰ê·  ì—”í‹°í‹°: {total_entities/chunks_with_entities:.1f}ê°œ")
    
    print(f"\n   ğŸ“š í† í”½ ë¶„ì„:")
    print(f"      - í† í”½ì´ ìˆëŠ” ì²­í¬: {chunks_with_topics}/{len(all_chunks)} ({chunks_with_topics/len(all_chunks)*100:.1f}%)")
    print(f"      - ì´ í† í”½ ìˆ˜: {total_topics}")
    print(f"      - ê³ ìœ  í† í”½ ìˆ˜: {len(all_topics)}")
    if chunks_with_topics > 0:
        print(f"      - ì²­í¬ë‹¹ í‰ê·  í† í”½: {total_topics/chunks_with_topics:.1f}ê°œ")
    
    # ì„ë² ë”© ìƒíƒœ í™•ì¸
    print(f"\n   ğŸ§  ì„ë² ë”© ìƒíƒœ:")
    embedding_statuses = {}
    for chunk in all_chunks:
        status = chunk.get('embedding_status', 'unknown')
        embedding_statuses[status] = embedding_statuses.get(status, 0) + 1
    
    for status, count in embedding_statuses.items():
        print(f"      - {status}: {count}ê°œ ({count/len(all_chunks)*100:.1f}%)")
    
    # ìƒìœ„ ì—”í‹°í‹°/í† í”½ í‘œì‹œ
    if all_entities:
        print(f"\n   ğŸŒŸ ìƒìœ„ ì—”í‹°í‹° (ìµœëŒ€ 20ê°œ):")
        entity_counts = {}
        for chunk in all_chunks:
            for entity in chunk.get('chunk_metadata', {}).get('entities', []):
                entity_counts[entity] = entity_counts.get(entity, 0) + 1
        
        sorted_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        for entity, count in sorted_entities:
            print(f"      - {entity}: {count}íšŒ")
    
    if all_topics:
        print(f"\n   ğŸ“– ìƒìœ„ í† í”½ (ìµœëŒ€ 20ê°œ):")
        topic_counts = {}
        for chunk in all_chunks:
            for topic in chunk.get('chunk_metadata', {}).get('topics', []):
                topic_counts[topic] = topic_counts.get(topic, 0) + 1
        
        sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        for topic, count in sorted_topics:
            print(f"      - {topic}: {count}íšŒ")
    
    print("\n" + "="*80)
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    
    client.close()

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    asyncio.run(analyze_spacy_chunks())