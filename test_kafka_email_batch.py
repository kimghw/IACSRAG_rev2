# test_kafka_email_batch.py
import asyncio
import json
import logging
from datetime import datetime
from typing import List, Dict, Any

from aiokafka import AIOKafkaConsumer
from infra.core.config import settings
from content_email.email_batch_processor import EmailBatchProcessor
from content_email.orchestrator import EmailOrchestrator

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Kafka ê´€ë ¨ ë¡œê·¸ ë ˆë²¨ ì¡°ì •
logging.getLogger("aiokafka").setLevel(logging.WARNING)
logging.getLogger("kafka").setLevel(logging.WARNING)

class TestEmailBatchProcessor:
    """í…ŒìŠ¤íŠ¸ìš© ì´ë©”ì¼ ë°°ì¹˜ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, batch_size: int = 3):
        self.batch_size = batch_size
        self.orchestrator = EmailOrchestrator()
        self.batch_processor = EmailBatchProcessor(
            batch_size=batch_size,
            orchestrator=self.orchestrator,
            max_wait_time=5.0  # 5ì´ˆ ëŒ€ê¸°
        )
        self.stats = {
            'total_events': 0,
            'total_batches': 0,
            'total_emails': 0
        }
    
    async def process_event(self, event_data: Dict[str, Any]):
        """ì´ë²¤íŠ¸ë¥¼ ë°°ì¹˜ í”„ë¡œì„¸ì„œì— ì¶”ê°€"""
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats['total_events'] += 1
        emails = event_data.get('response_data', {}).get('value', [])
        self.stats['total_emails'] += len(emails)
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“¥ Adding event to batch processor")
        logger.info(f"   - Event #{self.stats['total_events']}")
        logger.info(f"   - Event ID: {event_data.get('event_id')}")
        logger.info(f"   - Emails in event: {len(emails)}")
        logger.info(f"   - Current batch size: {len(self.batch_processor.current_batch)}/{self.batch_size}")
        
        # ë°°ì¹˜ í”„ë¡œì„¸ì„œì— ì¶”ê°€
        await self.batch_processor.add_event(event_data)
        
        # ë°°ì¹˜ê°€ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if len(self.batch_processor.current_batch) == 0:
            self.stats['total_batches'] += 1
            logger.info(f"âœ… Batch #{self.stats['total_batches']} processed!")
    
    def get_stats(self):
        """í˜„ì¬ í†µê³„ ë°˜í™˜"""
        return {
            **self.stats,
            'current_batch_size': len(self.batch_processor.current_batch),
            'batch_processor_stats': self.batch_processor.get_stats()
        }

async def test_kafka_email_batch():
    """Kafkaì—ì„œ ì´ë©”ì¼ ì´ë²¤íŠ¸ë¥¼ ê°€ì ¸ì™€ì„œ ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    
    # í…ŒìŠ¤íŠ¸ìš© ë°°ì¹˜ í”„ë¡œì„¸ì„œ ìƒì„±
    test_processor = TestEmailBatchProcessor(batch_size=3)
    
    # Kafka Consumer ìƒì„±
    consumer = AIOKafkaConsumer(
        settings.KAFKA_TOPIC_EMAIL_RECEIVED,
        bootstrap_servers=settings.KAFKA_BOOTSTRAP_SERVERS,
        group_id=f"test-batch-{datetime.now().timestamp()}",  # ìœ ë‹ˆí¬í•œ ê·¸ë£¹ ID
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        auto_offset_reset='earliest',  # ì²˜ìŒë¶€í„° ì½ê¸°
        enable_auto_commit=False,  # ìˆ˜ë™ ì»¤ë°‹ (í…ŒìŠ¤íŠ¸ìš©)
    )
    
    logger.info("ğŸš€ Starting Email Batch Processing Test")
    logger.info(f"ğŸ“¡ Kafka Topic: {settings.KAFKA_TOPIC_EMAIL_RECEIVED}")
    logger.info(f"ğŸ”§ Batch Size: {test_processor.batch_size}")
    logger.info(f"â±ï¸  Max Wait Time: {test_processor.batch_processor.max_wait_time}s")
    
    await consumer.start()
    
    try:
        max_events = 10  # ìµœëŒ€ 10ê°œ ì´ë²¤íŠ¸ ì²˜ë¦¬
        event_count = 0
        
        logger.info(f"\nğŸ“¨ Fetching up to {max_events} events from Kafka...")
        logger.info("=" * 80)
        
        async for msg in consumer:
            event_count += 1
            
            # ì´ë²¤íŠ¸ ì •ë³´ ì¶œë ¥
            event_data = msg.value
            event_id = event_data.get('event_id', 'unknown')
            account_id = event_data.get('account_id', 'unknown')
            emails = event_data.get('response_data', {}).get('value', [])
            
            logger.info(f"\nğŸ” Event {event_count}/{max_events} received")
            logger.info(f"   - Event ID: {event_id}")
            logger.info(f"   - Account ID: {account_id}")
            logger.info(f"   - Timestamp: {event_data.get('occurred_at')}")
            logger.info(f"   - Email count: {len(emails)}")
            
            # ì²« ë²ˆì§¸ ì´ë©”ì¼ ì •ë³´ ìƒ˜í”Œ
            if emails:
                first_email = emails[0]
                logger.info(f"   - Sample email:")
                logger.info(f"     â€¢ Subject: {first_email.get('subject', 'No subject')[:50]}...")
                logger.info(f"     â€¢ From: {first_email.get('from_address', {}).get('emailAddress', {}).get('address', 'Unknown')}")
                logger.info(f"     â€¢ Has attachments: {first_email.get('has_attachments', False)}")
            
            # ë°°ì¹˜ í”„ë¡œì„¸ì„œì— ì¶”ê°€
            await test_processor.process_event(event_data)
            
            # ìµœëŒ€ ì´ë²¤íŠ¸ ìˆ˜ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
            if event_count >= max_events:
                logger.info(f"\nğŸ Reached maximum events ({max_events}). Stopping...")
                break
        
        # ë‚¨ì€ ì´ë²¤íŠ¸ ì²˜ë¦¬
        if len(test_processor.batch_processor.current_batch) > 0:
            logger.info(f"\nâ³ Flushing remaining {len(test_processor.batch_processor.current_batch)} events...")
            await test_processor.batch_processor.flush()
            test_processor.stats['total_batches'] += 1
        
        # ìµœì¢… í†µê³„
        stats = test_processor.get_stats()
        batch_stats = stats['batch_processor_stats']
        
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š Test Summary:")
        logger.info(f"   - Total events processed: {stats['total_events']}")
        logger.info(f"   - Total emails processed: {stats['total_emails']}")
        logger.info(f"   - Total batches created: {stats['total_batches']}")
        logger.info(f"   - Batch processor stats:")
        logger.info(f"     â€¢ Batches processed: {batch_stats['total_batches_processed']}")
        logger.info(f"     â€¢ Emails processed: {batch_stats['total_emails_processed']}")
        logger.info(f"     â€¢ Failed batches: {batch_stats['failed_batches']}")
        logger.info(f"     â€¢ Average batch size: {batch_stats['average_batch_size']:.1f}")
        
        if event_count == 0:
            logger.warning("âš ï¸  No events found in Kafka topic!")
            
    except Exception as e:
        logger.error(f"âŒ Error during test: {str(e)}", exc_info=True)
        
    finally:
        await consumer.stop()
        logger.info("\nğŸ”Œ Kafka consumer stopped")

async def test_with_timeout():
    """íƒ€ì„ì•„ì›ƒ í…ŒìŠ¤íŠ¸ - 3ê°œ ë¯¸ë§Œì˜ ì´ë²¤íŠ¸ê°€ ìˆì„ ë•Œ"""
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ• Testing batch timeout behavior...")
    logger.info("This test will wait for the timeout if less than 3 events are available")
    
    # ì§§ì€ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    test_processor = TestEmailBatchProcessor(batch_size=3)
    test_processor.batch_processor.max_wait_time = 3.0  # 3ì´ˆë¡œ ë‹¨ì¶•
    
    # ë”ë¯¸ ì´ë²¤íŠ¸ 2ê°œ ìƒì„±
    for i in range(2):
        dummy_event = {
            'event_id': f'test-timeout-{i}',
            'account_id': 'test-account',
            'occurred_at': datetime.now().isoformat(),
            'response_data': {
                'value': [{
                    'id': f'email-{i}',
                    'subject': f'Test Email {i}',
                    'from_address': {
                        'emailAddress': {
                            'address': f'test{i}@example.com'
                        }
                    },
                    'body': {
                        'content': f'Test content {i}'
                    },
                    'has_attachments': False,
                    'is_read': False,
                    'importance': 'normal'
                }]
            }
        }
        
        logger.info(f"\nğŸ“§ Adding test event {i+1}/2")
        await test_processor.process_event(dummy_event)
    
    logger.info("\nâ³ Waiting for timeout (3 seconds)...")
    await asyncio.sleep(4)  # íƒ€ì„ì•„ì›ƒë³´ë‹¤ ì•½ê°„ ë” ëŒ€ê¸°
    
    stats = test_processor.get_stats()
    logger.info(f"\nğŸ“Š Timeout test results:")
    logger.info(f"   - Events added: 2")
    logger.info(f"   - Batches processed: {stats['total_batches']}")
    logger.info(f"   - Should be 1 batch processed after timeout")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸ¯ Email Batch Processing Test")
    logger.info(f"ğŸ“ Environment: {settings.KAFKA_CONSUMER_GROUP_ID}")
    logger.info(f"âš™ï¸  Batch Size: {settings.EMAIL_BATCH_SIZE}")
    logger.info(f"â° Max Wait Time: {settings.EMAIL_BATCH_MAX_WAIT_TIME}s")
    
    # ì„ íƒí•  í…ŒìŠ¤íŠ¸
    print("\ní…ŒìŠ¤íŠ¸ ì„ íƒ:")
    print("1. Kafkaì—ì„œ ì‹¤ì œ ì´ë²¤íŠ¸ ë°°ì¹˜ ì²˜ë¦¬")
    print("2. íƒ€ì„ì•„ì›ƒ ë™ì‘ í…ŒìŠ¤íŠ¸")
    print("3. ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    
    choice = input("\nì„ íƒ (1-3): ").strip()
    
    try:
        if choice == '1':
            await test_kafka_email_batch()
        elif choice == '2':
            await test_with_timeout()
        elif choice == '3':
            await test_kafka_email_batch()
            await test_with_timeout()
        else:
            logger.warning("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  Test interrupted by user")
    except Exception as e:
        logger.error(f"âŒ Test failed: {str(e)}", exc_info=True)
    
    logger.info("\nâœ… Test completed!")

if __name__ == "__main__":
    # .env íŒŒì¼ ë¡œë“œ
    from dotenv import load_dotenv
    import os
    
    env_file = '.env.development' if os.path.exists('.env.development') else '.env'
    load_dotenv(env_file)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(main())
