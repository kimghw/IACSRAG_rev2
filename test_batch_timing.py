# test_batch_timing.py
"""ë°°ì¹˜ ì²˜ë¦¬ íƒ€ì´ë° ìƒì„¸ ë¶„ì„"""

import asyncio
import sys
import os
import time
import logging
from datetime import datetime, timezone

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from content_pdf.services.batch_processor import BatchProcessor
from content_pdf.services.embedding_service import EmbeddingService
from content_pdf.services.storage_service import StorageService
from content_pdf.schema import ChunkDocument
from infra.core.config import settings

# ìƒì„¸ ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s.%(msecs)03d - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

class MockStorageService:
    """í…ŒìŠ¤íŠ¸ìš© ê°€ì§œ ì €ì¥ ì„œë¹„ìŠ¤"""
    async def save_chunks(self, chunks):
        await asyncio.sleep(0.1)  # ì €ì¥ ì‹œë®¬ë ˆì´ì…˜
        return [c.chunk_id for c in chunks]
    
    async def save_embeddings(self, embeddings, chunks):
        await asyncio.sleep(0.1)  # ì €ì¥ ì‹œë®¬ë ˆì´ì…˜

class TimingBatchProcessor(BatchProcessor):
    """íƒ€ì´ë° ì¸¡ì •ìš© ë°°ì¹˜ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.batch_timings = []
    
    async def _process_batch(self):
        """ë°°ì¹˜ ì²˜ë¦¬ with ìƒì„¸ íƒ€ì´ë°"""
        batch_num = len(self.batch_timings) + 1
        batch_size = len(self.current_batch)
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ”„ Processing Batch #{batch_num} ({batch_size} chunks)")
        logger.info(f"{'='*60}")
        
        batch_start = time.time()
        timing = {'batch_num': batch_num, 'size': batch_size}
        
        # 1. ì²­í¬ ì €ì¥
        save_start = time.time()
        chunk_ids = await self.storage_service.save_chunks(self.current_batch)
        timing['save_chunks'] = time.time() - save_start
        logger.info(f"  âœ“ Chunks saved: {timing['save_chunks']:.3f}s")
        
        # 2. ì„ë² ë”© ìƒì„±
        embed_start = time.time()
        embeddings = await self.embedding_service.generate_embeddings_batch(
            self.current_batch
        )
        timing['embeddings'] = time.time() - embed_start
        logger.info(f"  âœ“ Embeddings created: {timing['embeddings']:.3f}s")
        
        # 3. ë²¡í„° ì €ì¥
        vector_start = time.time()
        await self.storage_service.save_embeddings(embeddings, self.current_batch)
        timing['save_vectors'] = time.time() - vector_start
        logger.info(f"  âœ“ Vectors saved: {timing['save_vectors']:.3f}s")
        
        timing['total'] = time.time() - batch_start
        logger.info(f"  ğŸ“Š Batch total: {timing['total']:.3f}s")
        
        self.batch_timings.append(timing)
        self.current_batch = []
        self.stats['batches_processed'] += 1

async def test_batch_processing():
    """ë°°ì¹˜ ì²˜ë¦¬ íƒ€ì´ë° í…ŒìŠ¤íŠ¸"""
    
    logger.info(f"\nğŸ”§ Configuration:")
    logger.info(f"  - Batch size: {settings.PDF_BATCH_SIZE}")
    logger.info(f"  - Sub-batch size: {settings.PDF_SUB_BATCH_SIZE}")
    
    # ì„œë¹„ìŠ¤ ìƒì„±
    embedding_service = EmbeddingService()
    storage_service = MockStorageService()
    
    # ë‹¤ì–‘í•œ ì²­í¬ ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸
    test_cases = [30, 50, 62, 100]
    
    for total_chunks in test_cases:
        logger.info(f"\n\n{'#'*70}")
        logger.info(f"ğŸ“‹ Testing with {total_chunks} chunks")
        logger.info(f"{'#'*70}")
        
        # ë°°ì¹˜ í”„ë¡œì„¸ì„œ ìƒì„±
        processor = TimingBatchProcessor(
            batch_size=settings.PDF_BATCH_SIZE,
            embedding_service=embedding_service,
            storage_service=storage_service,
            document_id="test-doc"
        )
        
        # ì²­í¬ ìƒì„± ë° ì¶”ê°€ (ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜)
        overall_start = time.time()
        
        for i in range(total_chunks):
            chunk = ChunkDocument(
                document_id="test-doc",
                chunk_id=f"chunk-{i}",
                chunk_index=i,
                text_content=f"Test chunk {i} content " * 20,
                char_start=i*1000,
                char_end=(i+1)*1000,
                chunk_metadata={},
                created_at=datetime.now(timezone.utc)
            )
            
            # ì²­í¬ ì¶”ê°€
            await processor.add_chunk(chunk)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜ (ì•½ê°„ì˜ ë”œë ˆì´)
            if i % 10 == 0:
                await asyncio.sleep(0.001)
        
        # ë‚¨ì€ ì²­í¬ ì²˜ë¦¬
        logger.info(f"\nğŸ”„ Flushing remaining chunks...")
        await processor.flush()
        
        overall_time = time.time() - overall_start
        
        # ê²°ê³¼ ë¶„ì„
        logger.info(f"\n\nğŸ“Š Summary for {total_chunks} chunks:")
        logger.info(f"{'='*60}")
        logger.info(f"Total processing time: {overall_time:.3f}s")
        logger.info(f"Batches processed: {len(processor.batch_timings)}")
        
        total_embed_time = sum(t['embeddings'] for t in processor.batch_timings)
        logger.info(f"Total embedding time: {total_embed_time:.3f}s")
        logger.info(f"Average per chunk: {total_embed_time/total_chunks*1000:.1f}ms")
        
        # ë°°ì¹˜ë³„ ìƒì„¸ ì •ë³´
        logger.info(f"\nBatch details:")
        for t in processor.batch_timings:
            logger.info(f"  Batch {t['batch_num']}: {t['size']} chunks â†’ {t['embeddings']:.3f}s embedding")
        
        # ì˜ˆìƒ ë°°ì¹˜ ìˆ˜ vs ì‹¤ì œ
        expected_batches = (total_chunks + settings.PDF_BATCH_SIZE - 1) // settings.PDF_BATCH_SIZE
        logger.info(f"\nExpected batches: {expected_batches}")
        logger.info(f"Actual batches: {len(processor.batch_timings)}")

if __name__ == "__main__":
    asyncio.run(test_batch_processing())